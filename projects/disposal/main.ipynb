{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c3b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "import sys\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import gc\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append(\"/Users/xinc./Documents/GitHub/note\")    # for mac\n",
    "from module.get_info_Postgre import PostgresClient\n",
    "from module.get_info_FinMind import FinMindClient\n",
    "from module.plot_func import plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0aee6",
   "metadata": {},
   "source": [
    "# read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ebc628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取資料庫中的 disposal_info 表格\n",
    "database_name = \"disposal\"\n",
    "client = PostgresClient(database = database_name)\n",
    "disposal_info = client.fetch_table(table ='disposal_info', date_col='date', start_date='2018-01-01', end_date='2025-12-31', order_by='date')\n",
    "disposal_info = disposal_info.loc[disposal_info[\"is_disposal\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92998fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>is_disposal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1475</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2424</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>3219</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>911616</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>8287</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7641466</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>2515</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7641784</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>8358</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7641952</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>6488</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7642278</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>3163</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7642294</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>3081</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36512 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date stock_id  is_disposal\n",
       "121     2018-01-01     1475         True\n",
       "289     2018-01-01     2424         True\n",
       "1235    2018-01-01     3219         True\n",
       "1493    2018-01-01   911616         True\n",
       "1506    2018-01-01     8287         True\n",
       "...            ...      ...          ...\n",
       "7641466 2025-11-11     2515         True\n",
       "7641784 2025-11-11     8358         True\n",
       "7641952 2025-11-11     6488         True\n",
       "7642278 2025-11-11     3163         True\n",
       "7642294 2025-11-11     3081         True\n",
       "\n",
       "[36512 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disposal_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fetch_finmind_price",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 895 unique stocks with disposal events.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching by Stock: 100%|██████████| 895/895 [02:50<00:00,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 33450 price rows.\n",
      "        Date Stock_id   Open   High    Low  Close  Volume  TradingAmount\n",
      "0 2017-12-29     1475  48.00  53.30  48.00  50.00  901560       45481144\n",
      "1 2018-01-02     1475  48.20  48.20  45.00  45.00  169974        7779259\n",
      "2 2018-01-03     1475  45.10  48.00  45.10  45.85   87200        4039709\n",
      "3 2018-01-04     1475  46.95  47.95  46.00  46.80   65042        3053813\n",
      "4 2018-01-05     1475  46.00  46.30  45.75  46.05   52010        2386158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Log Suppression] 關閉 FinMind 的 loguru 輸出，避免洗版\n",
    "logger.remove()  # 移除預設 handler\n",
    "logger.add(sys.stderr, level=\"WARNING\") # 只顯示警告以上的訊息\n",
    "\n",
    "def fetch_and_filter_price(client, stock_id, event_dates, offset_days=3):\n",
    "    \"\"\"\n",
    "    針對單檔股票，抓取處置日前後指定天數的股價資料。\n",
    "    \"\"\"\n",
    "    # 決定抓取的時間範圍 (min - offset ~ max + offset)\n",
    "    min_date = event_dates.min() - timedelta(days=offset_days)\n",
    "    max_date = event_dates.max() + timedelta(days=offset_days)\n",
    "    \n",
    "    start_str = min_date.strftime('%Y-%m-%d')\n",
    "    end_str = max_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    try:\n",
    "        # 初始化並抓取資料\n",
    "        client.initialize_frame(stock_id=stock_id, start_time=start_str, end_time=end_str)\n",
    "        price_df = client.get_stock_price()\n",
    "        \n",
    "        if price_df.empty:\n",
    "            return None\n",
    "            \n",
    "        price_df = price_df.reset_index()\n",
    "        \n",
    "        # 建立篩選遮罩 (Mask)\n",
    "        # 只要日期落在任一處置事件的前後範圍內，即保留\n",
    "        mask = pd.Series(False, index=price_df.index)\n",
    "        for event_date in event_dates:\n",
    "            d_start = event_date - timedelta(days=offset_days)\n",
    "            d_end = event_date + timedelta(days=offset_days)\n",
    "            # 這裡使用 pd.to_datetime 確保型別一致\n",
    "            mask |= (price_df['Date'] >= pd.Timestamp(d_start)) & (price_df['Date'] <= pd.Timestamp(d_end))\n",
    "            \n",
    "        return price_df[mask].copy()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {stock_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# 準備資料與 Client\n",
    "disposal_events = disposal_info.copy()\n",
    "unique_stocks = disposal_events['stock_id'].unique()\n",
    "fm_client = FinMindClient()\n",
    "\n",
    "print(f\"Found {len(unique_stocks)} unique stocks with disposal events.\")\n",
    "\n",
    "all_prices = []\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# 迭代抓取\n",
    "for i, stock_id in enumerate(tqdm(unique_stocks, desc=\"Fetching by Stock\")):\n",
    "    # 找出該股票所有處置日期\n",
    "    stock_dates = disposal_events[disposal_events['stock_id'] == stock_id]['date']\n",
    "    \n",
    "    # 抓取並篩選\n",
    "    result_df = fetch_and_filter_price(fm_client, stock_id, stock_dates)\n",
    "    if result_df is not None:\n",
    "        all_prices.append(result_df)\n",
    "    \n",
    "    # [Memory Optimization] 定期回收記憶體\n",
    "    if i % BATCH_SIZE == 0:\n",
    "        gc.collect()\n",
    "\n",
    "# 最終整理\n",
    "if all_prices:\n",
    "    price_df = pd.concat(all_prices).drop_duplicates()\n",
    "    print(f\"Fetched {len(price_df)} price rows.\")\n",
    "    print(price_df.head())\n",
    "else:\n",
    "    print(\"No price data fetched.\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e98b30",
   "metadata": {},
   "source": [
    "## Analysis: Event Study on Disposal Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "595f1589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original events: 36512, Consolidated Start Events: 2192\n",
      "Event Study Data Prepared: 14272 rows\n",
      "        Date Stock_id t_label  gap_days  calendar_relative_day  Close\n",
      "0 2020-03-23   00642U     t-3       0.0                     -3  10.10\n",
      "1 2020-03-24   00642U     t-2       0.0                     -2  10.36\n",
      "2 2020-03-25   00642U     t-1       0.0                     -1  10.46\n",
      "3 2020-03-26   00642U     t+0       0.0                      0  10.33\n",
      "4 2020-03-27   00642U     t+1       0.0                      1  10.17\n",
      "5 2020-03-30   00642U     t+2       2.0                      4   9.79\n",
      "6 2020-03-31   00642U     t+3       0.0                      5  10.08\n",
      "7 2020-03-13   00672L     t-1       0.0                     -3   4.36\n",
      "8 2020-03-16   00672L     t+0       2.0                      0   4.03\n",
      "9 2020-03-17   00672L     t+1       0.0                      1   3.41\n"
     ]
    }
   ],
   "source": [
    "# --- Event Study Analysis (Corrected Gap Calculation) ---\n",
    "\n",
    "analysis_frames = []\n",
    "\n",
    "if 'price_df' in locals() and not price_df.empty:\n",
    "    # 1. 準備股價表\n",
    "    prices = price_df.sort_values(['Stock_id', 'Date']).copy()\n",
    "    prices['Date'] = pd.to_datetime(prices['Date'])\n",
    "    prices['trading_idx'] = prices.groupby('Stock_id').cumcount()\n",
    "    \n",
    "    # [Fix] 在這裡先計算 gap_days，避免合併後重複資料干擾\n",
    "    prices['prev_trade_date'] = prices.groupby('Stock_id')['Date'].shift(1)\n",
    "    prices['trade_date_diff'] = (prices['Date'] - prices['prev_trade_date']).dt.days\n",
    "    prices['gap_days'] = prices['trade_date_diff'].fillna(1) - 1\n",
    "    \n",
    "    # 2. 準備事件表 (Start Date Detection)\n",
    "    events = disposal_info[['stock_id', 'date']].sort_values(['stock_id', 'date'])\n",
    "    events = events.rename(columns={'stock_id': 'Stock_id', 'date': 'event_date'})\n",
    "    events['event_date'] = pd.to_datetime(events['event_date'])\n",
    "    \n",
    "    # 找出連續期間的起始日\n",
    "    events['prev_date'] = events.groupby('Stock_id')['event_date'].shift(1)\n",
    "    events['days_diff'] = (events['event_date'] - events['prev_date']).dt.days\n",
    "    is_start = events['days_diff'].isna() | (events['days_diff'] > 1)\n",
    "    start_events = events[is_start][['Stock_id', 'event_date']].copy()\n",
    "    \n",
    "    print(f\"Original events: {len(events)}, Consolidated Start Events: {len(start_events)}\")\n",
    "    \n",
    "    # 3. 找出事件日對應的交易日索引\n",
    "    event_indices = pd.merge(\n",
    "        start_events, \n",
    "        prices[['Stock_id', 'Date', 'trading_idx']], \n",
    "        left_on=['Stock_id', 'event_date'], \n",
    "        right_on=['Stock_id', 'Date'], \n",
    "        how='inner'\n",
    "    )[['Stock_id', 'event_date', 'trading_idx']].rename(columns={'trading_idx': 'event_idx'})\n",
    "    \n",
    "    # 4. 合併股價與事件索引\n",
    "    merged = pd.merge(prices, event_indices, on='Stock_id', how='inner')\n",
    "    \n",
    "    # 5. 計算相對天數 (Relative Trading Days)\n",
    "    merged['relative_day'] = merged['trading_idx'] - merged['event_idx']\n",
    "    \n",
    "    # 6. 計算自然日相對天數 (Calendar Relative Day)\n",
    "    merged['calendar_relative_day'] = (merged['Date'] - merged['event_date']).dt.days\n",
    "    \n",
    "    # 篩選範圍 (-3 ~ +3)\n",
    "    offset_days = 3\n",
    "    mask = (merged['relative_day'] >= -offset_days) & (merged['relative_day'] <= offset_days)\n",
    "    event_study_df = merged[mask].copy()\n",
    "    \n",
    "    # 7. t_label\n",
    "    def format_t(x):\n",
    "        if x > 0: return f't+{x}'\n",
    "        elif x < 0: return f't{x}'\n",
    "        else: return 't+0'\n",
    "    event_study_df['t_label'] = event_study_df['relative_day'].apply(format_t)\n",
    "    \n",
    "    # 8. 正規化 (Normalization)\n",
    "    base_prices = event_study_df[event_study_df['relative_day'] == -1][['Stock_id', 'event_date', 'Close', 'Volume']]\n",
    "    base_prices = base_prices.rename(columns={'Close': 'base_close', 'Volume': 'base_volume'})\n",
    "    \n",
    "    event_study_df = pd.merge(event_study_df, base_prices, on=['Stock_id', 'event_date'], how='left')\n",
    "    event_study_df['norm_close'] = event_study_df['Close'] / event_study_df['base_close']\n",
    "    event_study_df['norm_volume'] = event_study_df['Volume'] / event_study_df['base_volume']\n",
    "    \n",
    "    # 9. 整理欄位\n",
    "    cols = ['Date', 'Stock_id', 't_label', 'relative_day', 'gap_days', 'calendar_relative_day', 'Open', 'High', 'Low', 'Close', 'Volume', 'norm_close', 'norm_volume']\n",
    "    event_study_df = event_study_df[cols].sort_values(['Stock_id', 'Date'])\n",
    "    \n",
    "    print(f\"Event Study Data Prepared: {len(event_study_df)} rows\")\n",
    "    print(event_study_df[['Date', 'Stock_id', 't_label', 'gap_days', 'calendar_relative_day', 'Close']].head(10))\n",
    "else:\n",
    "    print(\"price_df not found or empty.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant_mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
